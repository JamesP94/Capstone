{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in CSV file\n",
    "survey_response = pd.read_csv('survey_response.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = survey_response['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(text):\n",
    "    # Typecast to string if text is not already a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    # Load English stop words from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert text to lowercase for case folding\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using a regular expression\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stop words from tokens\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts each comment through the process_text function\n",
    "processed_comments = [process_text(comment) for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.056*\"flight\" + 0.030*\"delays\" + 0.022*\"would\" + 0.016*\"communication\"')\n",
      "(1, '0.140*\"friendly\" + 0.138*\"staff\" + 0.093*\"flight\" + 0.064*\"time\"')\n",
      "(2, '0.163*\"service\" + 0.136*\"good\" + 0.108*\"great\" + 0.096*\"price\"')\n",
      "(3, '0.021*\"us\" + 0.017*\"terminal\" + 0.016*\"get\" + 0.015*\"back\"')\n",
      "(4, '0.127*\"everything\" + 0.103*\"went\" + 0.073*\"well\" + 0.045*\"trip\"')\n",
      "(5, '0.054*\"flight\" + 0.026*\"delayed\" + 0.022*\"hour\" + 0.022*\"plane\"')\n",
      "(6, '0.055*\"seats\" + 0.051*\"seat\" + 0.016*\"bag\" + 0.016*\"room\"')\n",
      "(7, '0.114*\"flight\" + 0.042*\"attendants\" + 0.037*\"plane\" + 0.036*\"attendant\"')\n",
      "(8, '0.098*\"sun\" + 0.093*\"country\" + 0.038*\"fly\" + 0.035*\"always\"')\n",
      "(9, '0.153*\"time\" + 0.082*\"flight\" + 0.065*\"nice\" + 0.050*\"flights\"')\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the processed comments\n",
    "dictionary = corpora.Dictionary(processed_comments)\n",
    "\n",
    "# Convert the dictionary to a Bag-of-Words corpus\n",
    "corpus = [dictionary.doc2bow(comment) for comment in processed_comments]\n",
    "\n",
    "# Apply the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
