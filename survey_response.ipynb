{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load survey response data\n",
    "survey_response = pd.read_csv('survey_response.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each value in the 'score' column\n",
    "score_counts = survey_response['score'].value_counts().sort_index()\n",
    "print(score_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ServiceStartDate' to datetime format\n",
    "survey_response['ServiceStartDate'] = pd.to_datetime(survey_response['ServiceStartDate'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to determine the season based on the month\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'fall'\n",
    "    else: # months 12, 1, 2\n",
    "        return 'winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the function to each row to create the 'season' column\n",
    "survey_response['season'] = survey_response['ServiceStartDate'].dt.month.apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the comment column to string data type from object data type\n",
    "survey_response['comment'] = survey_response['comment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all comments into a single string\n",
    "# You can separate each comment with a newline character for better readability\n",
    "all_comments = '\\n'.join(survey_response['comment'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the combined string to a text file\n",
    "with open('C:/Users/james/OneDrive/Documents/Capstone/Capstone/all_comments.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Typecast to string if text is not already a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    # Load English stop words from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert text to lowercase for case folding\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using a regular expression\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stop words from tokens\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all comments through the process_text function\n",
    "processed_all_comments= process_text(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Function that gives summary stats dictionary for a given text\n",
    "def get_patterns(text)  :\n",
    "    \"\"\"\n",
    "        This function takes text as an input and returns a dictionary of statistics,\n",
    "        after cleaning the text. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(text)\n",
    "    unique_tokens = set(text)\n",
    "    len_unique_tokens = len(unique_tokens)\n",
    "    avg_token_len = sum([len(token) for token in text]) / total_tokens if total_tokens > 0 else 0\n",
    "    lex_diversity = len_unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    top_20 = Counter(text).most_common(20)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':len_unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_20':top_20}\n",
    "\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all processed comments through the get_patterns function\n",
    "get_patterns(processed_all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for scores 4 and 5, and concatenate the comments\n",
    "high_score_comments = '\\n'.join(survey_response[survey_response['score'].isin([4, 5])]['comment'].astype(str))\n",
    "\n",
    "# Write the high score comments to a text file\n",
    "with open('C:/Users/james/OneDrive/Documents/Capstone/Capstone/high_score_comments.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(high_score_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame for scores 1, 2, and 3 and concatenate the comments\n",
    "low_score_comments = '\\n'.join(survey_response[survey_response['score'].isin([1, 2, 3])]['comment'].astype(str))\n",
    "\n",
    "# Write the low score comments to a text file\n",
    "with open('C:/Users/james/OneDrive/Documents/Capstone/Capstone/low_score_comments.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(low_score_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all high score comments through the process_text function\n",
    "processed_high_score_comments = process_text(high_score_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all low score comments through the process_text function\n",
    "processed_low_score_comments = process_text(low_score_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all processed high score comments through the get_patterns function\n",
    "get_patterns(processed_high_score_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all processed low score comments through the get_patterns function\n",
    "get_patterns(processed_low_score_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function to compare the relative use of words across two corpora\n",
    "def group_compare(corpus_1, corpus_2, num_words=10, ratio_cutoff=5):\n",
    "    sum_stats_corp_1 = get_patterns(corpus_1)\n",
    "    sum_stats_corp_2 = get_patterns(corpus_2)\n",
    "\n",
    "    # Extract word frequencies for both corpora\n",
    "    freq_1 = Counter(corpus_1)\n",
    "    freq_2 = Counter(corpus_2)\n",
    "\n",
    "    # Calculate ratios for words that appear at least ratio_cutoff times in both corpora\n",
    "    ratios_one_vs_two = {}\n",
    "    ratios_two_vs_one = {}\n",
    "\n",
    "    for word, count in freq_1.items():\n",
    "        if word in freq_2 and count >= ratio_cutoff and freq_2[word] >= ratio_cutoff:\n",
    "            p_1 = count / sum_stats_corp_1[\"tokens\"]\n",
    "            p_2 = freq_2[word] / sum_stats_corp_2[\"tokens\"]\n",
    "            ratios_one_vs_two[word] = p_1 / p_2\n",
    "            ratios_two_vs_one[word] = p_2 / p_1\n",
    "\n",
    "    # Sort and get top num_words for both ratios\n",
    "    top_ratios_one_vs_two = sorted(ratios_one_vs_two.items(), key=lambda x: x[1], reverse=True)[:num_words]\n",
    "    top_ratios_two_vs_one = sorted(ratios_two_vs_one.items(), key=lambda x: x[1], reverse=True)[:num_words]\n",
    "\n",
    "    results = {\n",
    "        \"one\": sum_stats_corp_1,\n",
    "        \"two\": sum_stats_corp_2,\n",
    "        \"one_vs_two\": dict(top_ratios_one_vs_two),\n",
    "        \"two_vs_one\": dict(top_ratios_two_vs_one)\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the word use relatively across high and low score comments\n",
    "group_compare(processed_high_score_comments, processed_low_score_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
